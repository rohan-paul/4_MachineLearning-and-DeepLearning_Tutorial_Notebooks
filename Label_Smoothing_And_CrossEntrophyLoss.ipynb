{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label smoothing\n",
    "\n",
    "Label smoothing, in a nutshell, is a way to make our model more robust so that it generalizes well and by doing this it avoids the problem of overconfidence.\n",
    "\n",
    "\n",
    "In essence, label smoothing will help your model to train around mislabeled data and consequently improve its robustness and performance.\n",
    "\n",
    "`label_smoothing` normally helps generalisation (test accuracy and AUC) in the presence of label noise\n",
    "\n",
    "\n",
    "### Problem of Overconfidence and its solution with Model Calibration\n",
    "\n",
    "An overconfident model is not calibrated and its predicted probabilities are consistently higher than the accuracy. For example, it may predict 0.9 for inputs where the accuracy is only 0.6. And note that models with small test errors can still be overconfident.\n",
    "\n",
    "Hence Model calibration is important for model interpretability and reliability\n",
    "\n",
    "A classification model is calibrated if its predicted probabilities of outcomes reflect their accuracy. For example, consider 100 examples within our dataset, each with predicted probability 0.9 by our model. If our model is calibrated, then 90 examples should be classified correctly. Similarly, among another 100 examples with predicted probabilities 0.6, we would expect only 60 examples being correctly classified.\n",
    "\n",
    "\n",
    "------------------------------------------------------------------\n",
    "\n",
    "### Formula of Label Smoothing\n",
    "\n",
    "Label smoothing replaces one-hot encoded label vector y_hot with a mixture of y_hot and the uniform distribution:\n",
    "\n",
    "\n",
    "## y_ls = (1 - α) * y_hot + α / K\n",
    "\n",
    "where K is the number of label classes, and α is a hyperparameter that determines the amount of smoothing. If α = 0, we obtain the original one-hot encoded y_hot. If α = 1, we get the uniform distribution.\n",
    "\n",
    "\n",
    "Example application of above formulae\n",
    "\n",
    "Say you were training a model for binary classification. Your labels would be 0 — cat, 1 — not cat.\n",
    "\n",
    "Now, say you label_smoothing = 0.2\n",
    "\n",
    "Using the equation above, we get:\n",
    "\n",
    "```\n",
    "new_onehot_labels = [0 1] * (1 — 0.2) + 0.2 / 2 =[0 1]*(0.8) + 0.1\n",
    "\n",
    "new_onehot_labels =[0.9 0.1]\n",
    "\n",
    "```\n",
    "\n",
    "These are soft labels, instead of hard labels, that is 0 and 1. This will ultimately give you lower loss when there is an incorrect prediction, and subsequently, your model will penalize and learn incorrectly by a slightly lesser degree.\n",
    "\n",
    "In other words, instead of using the hard labels or the one-hot encoded variables where the true label is 1, let’s replace them with (1-α) * 1 where α refers to the smoothing parameter. Once that’s done, we add some uniform noise 1/K to the labels where K: total number of labels.\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "The Problem in without Label-Smoothing for a multi-class classification problem is -\n",
    "\n",
    "For the cross-Entropy loss to really be at a minimum, each logit corresponding to the correct class needs to be significantly higher than the rest. That is, for example for row-1, img-1.jpg the logit of 4.7 corresponding to is_dog needs to be significantly higher than the rest. This is also the case for all the other rows.\n",
    "\n",
    "#### A mathematical proof of this problem was presented in a [Paper by Lei Mao](https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/) where he explains why minimizing cross entropy loss is equivalent to do maximum likelihood estimation.\n",
    "\n",
    "\n",
    "The above can cause two problems. First, it may result in over-fitting: if the model learns to assign full probability to the ground- truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large.\n",
    "\n",
    "In other words, our model could become overconfident of it’s predictions because to really minimise the loss, our model needs to be very sure of everything that it predicts. This is bad because it is then harder for the model to generalise and easier for it to overfit to the training data.\n",
    "\n",
    "Intuitively, label smoothing restraints the logit value for the correct class to be closer to the logit values for other classes.\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "## Formulae for Label Smoothing Cross Entropy loss\n",
    "\n",
    "\n",
    "Label Smoothing is designed to make the model a little bit less certain of it’s decision by changing a little bit its target:\n",
    "\n",
    "#### So, instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as:\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/mwo8Tfl.png)\n",
    "\n",
    "i.e.\n",
    "\n",
    "![Imgur](https://imgur.com/3AQ3Ns5.png))\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Fastai/PyTorch Implementation of Label Smoothing Cross Entropy loss\n",
    "\n",
    "https://towardsdatascience.com/label-smoothing-as-another-regularization-trick-7b34c50dc0b9\n",
    "\n",
    "Label smoothing changes the target vector by a small amount ε. Thus, instead of asking our model to predict 1 for the right class, we ask it to predict 1-ε for the correct class and ε for all the others. So, the cross-entropy loss function with label smoothing is transformed into the formula below.\n",
    "\n",
    "![](assets/2022-02-19-01-46-14.png)\n",
    "\n",
    "In this formula, ce(x) denotes the standard cross-entropy loss of x (e.g. -log(p(x))), ε is a small positive number, i is the correct class and N is the number of classes.\n",
    "\n",
    "\n",
    "```py\n",
    "\n",
    "class LabelSmoothingCrossEntropy(Module):\n",
    "    y_int = True\n",
    "    def __init__(self, eps:float=0.1, reduction='mean'): self.eps,self.reduction = eps,reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        if self.reduction=='sum': loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=-1) #We divide by that size at the return line so sum and not mean\n",
    "            if self.reduction=='mean':  loss = loss.mean()\n",
    "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "### Example of the Problem (without Label Smoothing)\n",
    "\n",
    "Suppose we have K = 3 classes, and our label belongs to the 1st class. Let [a, b, c] be our logit vector.\n",
    "If we do not use label smoothing, the label vector is the one-hot encoded vector [1, 0, 0]. Our model will make a ≫ b and a ≫ c.\n",
    "\n",
    "For example,\n",
    "\n",
    "My logit vector [10, 0, 0] => Applying softmax to to this gives\n",
    "\n",
    "[0.9999, 0, 0] rounded to 4 decimal places.\n",
    "\n",
    "If we use label smoothing with α = 0.1,\n",
    "\n",
    "The logit vector becomes [3.3322, 0, 0]\n",
    "\n",
    "And the smoothed label vector after softmax becomes [0.9333, 0.0333, 0.0333].\n",
    "\n",
    "So the smoothed label vector after softmax, has a smaller gap.\n",
    "\n",
    "This is why we call label smoothing a regularization technique as it restrains the largest logit from becoming much bigger than the rest.\n",
    "\n",
    "----------------------\n",
    "\n",
    "### Q: When do we use label smoothing?\n",
    "A: Whenever a classification neural network suffers from overfitting and/or overconfidence, we can try label smoothing.\n",
    "\n",
    "### Q: How do we choose α?\n",
    "A: Just like other regularization hyperparameters, there is no formula for choosing α. It is usually done by trial and error, and α = 0.1 is a good place to start.\n",
    "\n",
    "### Q: Can we use distributions other than uniform distribution in label smoothing?\n",
    "A: Technically yes. In [4] the theoretical groundwork is developed for arbitrary distributions. That being said, the vast majority of empirical studies on label smoothing use uniform distribution.\n",
    "\n",
    "###  Q: Is label smoothing used outside deep learning?\n",
    "\n",
    "A: Not really. Most popular non-deep learning methods do not use the softmax function. Thus label smoothing is usually not applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label-Smoothing-CrossEntropyLoss from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulae for Label Smoothing Cross Entropy loss\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/OADA4gm.png)\n",
    "\n",
    "![Imgur](https://imgur.com/PuwOVQk.png)\n",
    "\n",
    "\n",
    "Label Smoothing is designed to make the model a little bit less certain of it’s decision by changing a little bit its target:\n",
    "\n",
    "\n",
    "#### So, instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(torch.nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1, \n",
    "                 reduction=\"mean\", weight=None):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.epsilon   = epsilon\n",
    "        self.reduction = reduction\n",
    "        self.weight    = weight\n",
    "\n",
    "    def reduce_loss(self, loss):\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n",
    "         if self.reduction == 'sum' else loss\n",
    "\n",
    "    def linear_combination(self, i, j):\n",
    "        return (1 - self.epsilon) * i + self.epsilon * j\n",
    "\n",
    "    def forward(self, predict_tensor, target):\n",
    "        assert 0 <= self.epsilon < 1\n",
    "\n",
    "        if self.weight is not None:\n",
    "            self.weight = self.weight.to(predict_tensor.device)\n",
    "\n",
    "        num_classes = predict_tensor.size(-1)\n",
    "        \n",
    "        log_preds = F.log_softmax(predict_tensor, dim=-1)\n",
    "        \n",
    "        loss = self.reduce_loss(-log_preds.sum(dim=-1))\n",
    "        \n",
    "        negative_log_likelihood_loss = F.nll_loss(\n",
    "            log_preds, target, reduction=self.reduction, weight=self.weight\n",
    "        )\n",
    "        return self.linear_combination(negative_log_likelihood_loss, loss / num_classes,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The negative log likelihood loss - `torch.nn.functional.nll_loss`\n",
    "\n",
    "\n",
    "The cross-entropy loss and the (negative) log-likelihood are\n",
    "the same in the following sense:\n",
    "\n",
    "If you apply Pytorch’s CrossEntropyLoss to your output layer,\n",
    "you get the same result as applying Pytorch’s NLLLoss to a\n",
    "LogSoftmax layer added after your original output layer.\n",
    "\n",
    "(I suspect – but don’t know for a fact – that using\n",
    "CrossEntropyLoss will be more efficient because it\n",
    "can collapse some calculations together, and doesn’t\n",
    "introduce an additional layer.)\n",
    "\n",
    "You are trying to maximize the “likelihood” of your model\n",
    "parameters (weights) having the right values. Maximizing\n",
    "the likelihood is the same as maximizing the log-likelihood,\n",
    "which is the same as minimizing the negative-log-likelihood.\n",
    "For the classification problem, the cross-entropy is the\n",
    "negative-log-likelihood. \n",
    "\n",
    "Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/pf8iEb8.png)\n",
    "\n",
    "From the above we can see that if we want to minimize the Cross-Entropy (cross-entropy loss in many deep learning libraries) we need to minimize the Negative Log-Likelihood of the model (cross-entropy loss in many libraries typically calculate Negative Log-Likelihood Loss and Log-Softmax under the hood, like in PyTorch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4670)\n"
     ]
    }
   ],
   "source": [
    "loss_criterion = LabelSmoothingLoss(epsilon=0.5)\n",
    "\n",
    "predict_tensor = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                                [0, 0.9, 0.2, 0.2, 1], \n",
    "                                [1, 0.2, 0.7, 0.9, 1]])\n",
    "\n",
    "target = Variable(torch.LongTensor([2, 1, 0]))\n",
    "\n",
    "loss_label_smoothed = loss_criterion(Variable(predict_tensor), target )\n",
    "\n",
    "print(loss_label_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tensor.size(-1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
