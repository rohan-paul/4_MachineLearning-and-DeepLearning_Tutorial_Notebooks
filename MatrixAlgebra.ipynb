{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Basic Definitions\n",
    "\n",
    "A matrix **A** over a field **K** or, simply, a matrix A (when K is implicit) is a rectangular array of scalars usually presented in the following form:\n",
    "\n",
    "![](https://imgur.com/T6W9azd.png)\n",
    "\n",
    "\n",
    "A matrix with m rows and n columns is called an m by n matrix, written as m*n. \n",
    "\n",
    "The pair of numbers m and n is called the size of the matrix. Two matrices A and B are equal, written A = B, if they have the same size and if corresponding elements are equal. Thus, the equality of two m*n matrices is equivalent to a system of mn equalities, one for each corresponding pair of elements.\n",
    "\n",
    "![](https://imgur.com/VJzaGub.png)\n",
    "\n",
    "\n",
    "**Matrix vs Vectors**\n",
    "\n",
    "A matrix is simply a rectangular array of numbers and a vector is a row (or column) of a matrix.\n",
    "\n",
    "vector is one dimension array such a=[1 2 3 4 5], but the matrix is more than one dimension array, and has some of operations.\n",
    "\n",
    "A real system of linear equations\n",
    "\n",
    "![](https://imgur.com/ssBXr2S.png)\n",
    "\n",
    "\n",
    "### **The rank of a matrix**\n",
    "\n",
    "![](https://imgur.com/FnW0D93.png)\n",
    "\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "![](https://imgur.com/inT4RkS.png)\n",
    "\n",
    "\n",
    "**There are four simple rules that will help us in multiplying matrices, listed here:**\n",
    "\n",
    "1. Firstly, we can only multiply two matrices when the number of columns in matrix A is equal to the number of rows in matrix B.\n",
    "\n",
    "2. Secondly, the first row of matrix A multiplied by the first column of matrix B gives us the first element in the matrix AB, and so on.\n",
    "\n",
    "3. Thirdly, when multiplying, order matters — specifically, AB ≠ BA.\n",
    "\n",
    "4. Lastly, the element at row i, column j is the product of the ith row of matrix A and the jth column of matrix B.\n",
    "\n",
    "Further read through [this](https://www.mathsisfun.com/algebra/matrix-multiplying.html) for a very nice visual flow of Matrix Multiplication.\n",
    "\n",
    "Let A; B; C be matrices. Then, whenever the products and sums are defined,\n",
    "\n",
    "![](https://imgur.com/XbdWWqH.png)\n",
    "\n",
    "\n",
    "### Square Matrices\n",
    "\n",
    "A square matrix is a matrix with the same number of rows as columns. An n *n square matrix is said to be of order n and is sometimes called an n-square matrix.\n",
    "Recall that not every two matrices can be added or multiplied. However, if we only consider square matrices of some given order n, then this inconvenience disappears. Specifically, the operations of addition, multiplication, scalar multiplication, and transpose can be performed on any n*n matrices, and\n",
    "the result is again an n — n matrix.\n",
    "\n",
    "The following are square matrices of order 3\n",
    "\n",
    "![](https://imgur.com/5xc8Uui.png)\n",
    "\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "For an n × n matrix A, if the linear algebraic equation\n",
    "\n",
    "![](https://imgur.com/tJ1SeD9.png)\n",
    "\n",
    "\n",
    "**has a nonzero n × 1 solution vector u, then the scalar λ is called an eigenvalue of the matrix A, and u is its eigenvector corresponding to λ**\n",
    "\n",
    "Eigenvalues and eigenvectors are only for square matrices. Non-square matrices do not have eigenvalues. If the matrix A is a real matrix, the eigenvalues will either be all real, or else there will be complex conjugate pairs.\n",
    "\n",
    "Eigenvectors are _by definition nonzero_. Eigenvalues may be equal to zero.\n",
    "\n",
    "We do not consider the zero vector to be an eigenvector: since A0=0=λ0 for _every_ scalar λ, the associated eigenvalue would be .\n",
    "\n",
    "If someone hands you a matrix A and a vector v, it is easy to check if v is an eigenvector of A: simply multiply v by A and see if Av is a scalar multiple of v. On the other hand, given just the matrix A it is not so straightforward to find the eigenvectors. There are few steps involved here.\n",
    "\n",
    "### MATRIX FACTORIZATION\n",
    "\n",
    "First note, most of the times “matrix factorization” and “matrix decomposition” are used interchangeably\n",
    "\n",
    "Suppose we want to express an m * n matrix A as the product of two matrices A1 and A2 in the form\n",
    "\n",
    "![](https://imgur.com/pYgTyss.png)\n",
    "\n",
    "\n",
    "for any matrices A and B for which their product can be defined.\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "Singular value decomposition is a method of decomposing a matrix into three other matrices and its a central matrix decomposition method in linear algebra. It has been referred to as the “fundamental theorem of linear algebra”.\n",
    "\n",
    "![](https://imgur.com/YoiZzSm.png)\n",
    "\n",
    "\n",
    "The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning. SVD can also be used in least squares linear regression, image compression, and denoising data.\n",
    "\n",
    "Here are the dimensions of the factorization:\n",
    "\n",
    "![](https://imgur.com/Lj91W2z.png)\n",
    "\n",
    "[image source](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/)\n",
    "\n",
    "**Interpretation of SVD**\n",
    "\n",
    "The intuition behind the singular value decomposition needs some explanations about the idea of matrix transformation.\n",
    "\n",
    "**A** is a matrix that can be seen as a linear transformation. This transformation can be decomposed in three sub-transformations: 1. rotation, 2. re-scaling, 3. rotation. These three steps correspond to the three matrices **U**, **Σ**, and **V**.\n",
    "\n",
    "#### Comparing SVD with Eigenvalues and Eigen Vectors\n",
    "\n",
    "First noting the formulae of Eigenvalues\n",
    "\n",
    "![](https://imgur.com/MNM7I6S.png)\n",
    "\n",
    "The key point to note here, that the concept of Singular-Value is a lot like eigenvalues, but different because the matrix A now is more usually rectangular. But for a rectangular matrix, the whole idea of eigenvalues\n",
    "is not possible because if I multiply A times a vector x in n dimensions, out will come something in m dimensions and it’s not going to equal lambda x.\n",
    "\n",
    "So Ax equal lambda x is not even possible if A is rectangular. And here comes SVD and so this is the new word is singular. And in between go the — \n",
    "not the eigenvalues, but the singular values. There are two sets of singular vectors, not one. For eigenvectors, we just had one set.\n",
    "\n",
    "We’ve got one set of left eigenvectors in m dimensions, and we’ve got another set of right eigenvectors in n dimensions. And numbers in between are not eigenvalues, but singular values.\n",
    "\n",
    "### Derivatives of Scalars with Respect to Vectors; The Gradient\n",
    "\n",
    "The derivative of a scalar-valued function with respect to a vector is a vector\n",
    "of the partial derivatives of the function with respect to the elements of the\n",
    "vector. If f (x) is a scalar function of the vector x = (x1 , . . . , xn ),\n",
    "\n",
    "![](https://imgur.com/SAlG9uo.png)\n",
    "\n",
    "\n",
    "The notation $g_f$ or ∇f implies differentiation with respect to “all” arguments of f , hence, if f is a scalar-valued function of a vector argument, they represent a vector. This derivative is useful in finding the maximum or minimum of a function. Such applications arise throughout statistical and numerical analysis.\n",
    "\n",
    "### Derivatives of Vectors with Respect to Vectors; The Jacobian\n",
    "\n",
    "The derivative of an m-vector-valued function of an n-vector argument con-\n",
    "sists of nm scalar derivatives. These derivatives could be put into various structures. Two obvious structures are an n × m matrix and an m × n matrix.\n",
    "\n",
    "![](https://imgur.com/n8hH2Mh.png)\n",
    "\n",
    "\n",
    "### Higher-Order Derivatives with Respect to Vectors; The Hessian\n",
    "\n",
    "Higher-order derivatives are derivatives of lower-order derivatives. As we have seen, a derivative of a given function with respect to a vector is a more complicated object than the original function. The simplest higher-order derivative with respect to a vector is the second-order derivative of a scalar-valued function. Higher-order derivatives may become uselessly complicated.\n",
    "In accordance with the meaning of derivatives of vectors with respect to\n",
    "vectors, the second derivative of a scalar-valued function with respect to a\n",
    "vector is a matrix of the partial derivatives of the function with respect to the\n",
    "elements of the vector. This matrix is called the Hessian, and is denoted by\n",
    "Hf or sometimes by ∇∇f or ∇^2f :\n",
    "\n",
    "![](https://imgur.com/BTwXpSh.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
