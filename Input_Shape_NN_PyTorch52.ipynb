{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Shape of Tensor in Neural Network for PyTorch\n",
    "\n",
    "------\n",
    "\n",
    "### Understanding `nn.Linear()` Layer\n",
    "\n",
    "This takes 2 parameters. input features and output features, which are the number of inputs and number of outputs.\n",
    "This will create a weight matrix and bias vector randomly.\n",
    "\n",
    "```\n",
    "fc = nn.Linear(in_features=4, out_features=3, bias=False)\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    " - **in_features** – size of each input sample (i.e. size of x)\n",
    " - **out_features** – size of each output sample (i.e. size of y)\n",
    " - **bias** – If set to False, the layer will not learn an additive bias. Default: True\n",
    "\n",
    "------------------\n",
    "\n",
    "Note that the weights `W` have shape `(out_features, in_features)` and biases `b` have shape `(out_features)`. They are initialized randomly and can be changed later (e.g. during the training of a Neural Network they are updated by some optimization algorithm).\n",
    "\n",
    "\n",
    "in the above line we have. We have defined a linear layer that accepts 4 input features and converts them into 3 output features, so we convert from 4 dimensional space to 3 dimensional space. We know that a weight matrix is ​​needed to perform this operation, but where is the weight matrix in this example?\n",
    "\n",
    "\n",
    "The PyTorch LinearLayer class uses the numbers 4 and 3 passed to the constructor to create a 3 x 4 weight matrix. Let's verify this by looking at the PyTorch source code.\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L79\n",
    "\n",
    "```py\n",
    "def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2d() Layer\n",
    "\n",
    "The syntax must be like this.\n",
    "\n",
    "#### a = Conv2d(in_channels, out_channels, kernel_size=(n, n), stride, padding, bias)\n",
    "\n",
    "Parameters\n",
    "* in_channels (int) - Number of channels in the input image\n",
    "* out_channels (int) - Number of channels produced by the convolution\n",
    "\n",
    "In Conv2d, you define input/output channel and kernel size and some arbitrary args like padding, not output size. Output size will be determined using kernel_size, stride, padding, etc.\n",
    "\n",
    "An image in PyTorch has thre dimensions [channel, height, width]. So for a RGB image, [3, height, width]. If you want to get a 3 channel image as the result, you need to use a convolution that takes images with same channel size of your input which is 3, and 3 channels as the output, nn.Conv2d(3, 3, kernel_size) where kernel_size is the arbitrary size for filters.\n",
    "\n",
    "Conv2d needs 2D kernels with 1 channel (grayscale mode, 3 in RGB). For having outputs with more than one, you need to run conv2d out_channel times using [1, k, k] size kernels so the result will be like [out_channel, h, w] because all the respones to out_channel different [1, k, k] kernels have been concatenated.\n",
    "\n",
    "For instance, assume a case that your input image has 10 dimensions [batch_size, 10, h, w] and you want to have 3 as output channel, [batch_size, 3, h, w]. In this case, we need 3 different filters that each has size of [10, k, k]. Each one will create a output with size of [batch_size, 1, h, w] and finally all will be concatenated to have a output [batch, 3, h, w]. So, the kernel size in this case would be [3, 10, k, k].\n",
    "\n",
    "-----------\n",
    "\n",
    "\n",
    "### Various Tensor sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 5, 15, 15])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.Size([16])\n",
    "    # 1d Tensor : [batch_size]\n",
    "    # used for target labels or predictions.\n",
    "torch.Size([16, 256])\n",
    "    # 2D- Tensor : [batch_size, num_features (aka: C * H * W)]\n",
    "    # use for nn.Linear() input.\n",
    "torch.Size([10, 1, 2048])\n",
    "    # 3-D Tensor : [batch_size, channels, num_features (aka: H * W)]\n",
    "    # when used as nn.Conv1d() input.\n",
    "    # (but [seq_len, batch_size, num_features]\n",
    "    # if feeding an RNN).\n",
    "torch.Size([16, 3, 28, 28])\n",
    "    # 4-D Tensor : [batch_size, channels, height, width]\n",
    "    # use for nn.Conv2d() input.\n",
    "torch.Size([32, 1, 5, 15, 15])\n",
    "    # 5D-Tensor: [batch_size, channels, depth, height, width]\n",
    "    # use for nn.Conv3d() input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The key step is between the last convolution and the first `Linear` block.\n",
    "\n",
    "- `Conv2d` outputs a tensor of shape `[batch_size, n_features_conv, height, width]` whereas\n",
    "\n",
    "- `Linear` expects `[batch_size, n_features_lin]`.\n",
    "\n",
    "\n",
    "To make the two align you need to \"stack\" the 3 dimensions `[n_features_conv, height, width]` into one `[n_features_lin]`. As follows, it must be that `n_features_lin == n_features_conv * height * width`. In the original code this \"stacking\" is achieved by\n",
    "\n",
    "    x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "\n",
    "### Most Important Rule while Transitioning from Conv2D Layer to a Linear Layer\n",
    "\n",
    "## For transitioning from a convolutional layer output to a linear layer input - I must resize Conv Layer output which is a 4d-Tensor to a 2d-Tensor using view.\n",
    "\n",
    "#### An example, a conv output of [32, 21, 50, 50] should be “flattened” to become a [32, 21 * 50 * 50] tensor. \n",
    "\n",
    "#### And the in_features of the linear layer should also be set to [21 * 50 * 50].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "\n",
    "### Calculate the dimensions for nn.Linear()\n",
    "\n",
    "\n",
    "The key step is between the last convolution and the first `Linear` block.\n",
    "\n",
    "- `Conv2d` outputs a tensor of shape `[batch_size, n_features_conv, height, width]` whereas\n",
    "\n",
    "- `Linear` expects `[batch_size, n_features_lin]`.\n",
    "\n",
    "There are two, specifically important arguments for all nn.Linear layer networks that you should be aware of no matter how many layers deep your network is. The very first argument, and the very last argument.\n",
    "\n",
    "----------------\n",
    "\n",
    "#### If you want to pass in your 28 x 28 image into a linear layer, you have to know two things:\n",
    "\n",
    "#### Your 28 x 28 pixel image can’t be input as a [28, 28] tensor. This is because nn.Linear will read it as 28 batches of 28-feature-length vectors. Since it expects an input of [batch_size, num_features], you have to transpose it somehow.\n",
    "\n",
    "\n",
    "Your batch size passes unchanged through all your layers. No matter how your data changes as it passes through a network, your first dimension will end up being your batch_size even if you never see that number explicitly written anywhere in your network module’s definition.\n",
    "Use view() to change your tensor’s dimensions.\n",
    "\n",
    "`image = image.view(batch_size, -1)`\n",
    "\n",
    "You supply your batch_size as the first number, and then “-1” basically tells Pytorch, “you figure out this other number for me… please.” Your tensor will now feed properly into any linear layer. Now we’re talking!\n",
    "\n",
    "So then, to initialize the very first argument of your linear layer, pass it the number of features of your input data. For 28 x 28, our new view tensor is of size [1, 784] (1 * 28 * 28):\n",
    "Example 3: Resize with view() to fit into a linear layer\n",
    "\n",
    "```py\n",
    "batch_size = 1\n",
    "# Simulate a 28 x 28 pixel, grayscale \"image\"\n",
    "input_image = torch.randn(1, 28, 28)\n",
    "\n",
    "\n",
    "# Use view() to get [batch_size, num_features].\n",
    "# -1 calculates the missing value given the other dim.\n",
    "input_image = input_image.view(batch_size, -1) # torch.Size([1, 784])\n",
    "\n",
    "\n",
    "# Intialize the linear layer.\n",
    "fc = torch.nn.Linear(784, 10)\n",
    "# Pass in the simulated image to the layer.\n",
    "\n",
    "output = fc(input)\n",
    "print(output.shape)\n",
    "\n",
    "# torch.Size([1, 10])\n",
    "```\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "Hence the reason that the linear cannot be placed in the same Sequential function right after the conv layers. The reason is the input to linear needs to be reshaped. So you have to create two sections, one Sequntial for all the conv layers (and their pooling, and activations, …), and then the output of this is reshaped to a flat tensor of shape batch-size x ?, and then this is passed to the linear layers.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Also note the following \n",
    "\n",
    "`Linear (50*50, 64)` is exactly the same as `Linear (2500, 64)`. \n",
    "\n",
    "The fact that you write the number 2500 as 50*50 does not somehow tell the layer to accept an input of shape [50, 50].)\n",
    "\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from PyTorch https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "#### While Transitioning from a convolutional layer output to a linear layer input\n",
    "\n",
    "our batch size passes unchanged through all your layers. No matter how your data changes as it passes through a network, your first dimension will end up being your batch_size even if you never see that number explicitly written anywhere in your network module’s definition.\n",
    "Use view() to change your tensor’s dimensions.\n",
    "\n",
    "image = image.view(batch_size, -1)\n",
    "\n",
    "You supply your batch_size as the first number, and then “-1” basically tells Pytorch, “you figure out this other number for me… please.” Your tensor will now feed properly into any linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-D Tensor Shape of x BEFORE Linear layer & before reshaping  torch.Size([32, 16, 5, 5])\n",
      "Required Shape Conversion of x to a 2-D Tensor for feeding Linear layer  torch.Size([32, 400])\n",
      "Shape of x AFTER re-shaping before feeding to first Linear layer  torch.Size([32, 400])\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension. And 16*5*5 gives me 400\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        print('4-D Tensor Shape of x BEFORE Linear layer & before reshaping ', x.shape)\n",
    "        print('Required Shape Conversion of x to a 2-D Tensor for feeding Linear layer ', (x.view(x.shape[0], -1)).shape)             \n",
    "        # x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        print('Shape of x AFTER re-shaping before feeding to first Linear layer ', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "input = torch.randn(32, 1, 32, 32)\n",
    "output = net(input)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
